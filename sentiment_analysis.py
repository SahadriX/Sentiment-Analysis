# -*- coding: utf-8 -*-
"""SENTIMENT ANALYSIS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PzyN_XttxZv8fwEorZGzfa9r2MvOapnx
"""

import numpy as np
import pandas as pd
import nltk#This line of code imports the nltk library, which contains tools and resources for natural language processing tasks
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize#This specifies that you want to import something from the nltk.tokenize module, which is part of the NLTK library
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer# used to convert a collection of raw documents into a matrix of TF-IDF features
#: It combines TF and IDF to assign a weight to each word in each document. This weight represents the importance of the word in that specific document and in the context of the entire collection.
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import re#This is the name of the module that provides regular expression operations in Python. Regular expressions are essentially a powerful way to search for and manipulate text based on patterns.
from bs4 import BeautifulSoup#This specifies that you want to import something from the bs4 library, which is BeautifulSoup4.
# This imports the BeautifulSoup class, which is the main tool for parsing HTML and XML documents.

# Download necessary NLTK data
nltk.download('punkt')#This line triggers the download of the 'punkt' resource. This resource is a pre-trained model for sentence tokenization.
nltk.download('stopwords')
nltk.download('wordnet')

# Load the dataset
df = pd.read_csv('/content/IMDB_Dataset_Fixed (1).csv')

# Handle missing values
df = df.dropna(subset=['review', 'sentiment'])#This line of code is used to remove rows from a Pandas DataFrame (df) where there are missing values (NaN) in either the 'review' or 'sentiment' columns.

def clean_text(text):
    """Function to clean text data"""
    soup = BeautifulSoup(text, "html.parser")#This line creates a BeautifulSoup object named soup from the input text using the html.parser.
#BeautifulSoup is a library used for parsing HTML and XML documents. It helps extract data from web pages and clean up messy HTML.
    text = soup.get_text()#This line extracts all the text from the soup object (the parsed HTML) and assigns it back to the text variable.
    text = re.sub(r'[^a-zA-Z\s]', '', text)#This line uses regular expressions (re module) to remove any characters that are not letters (a-z, A-Z) or whitespace (\s).
    text = text.lower()
    return text

df['cleaned_review'] = df['review'].apply(clean_text)#This part creates a new column in your DataFrame called 'cleaned_review'. It will store the output of the apply() method.

# Initialize NLP tools
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [stemmer.stem(word) for word in tokens]  # Stemming
    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization
    return ' '.join(tokens)

df['processed_review'] = df['cleaned_review'].apply(preprocess_text)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(df['processed_review'], df['sentiment'], test_size=0.2, random_state=42)

# Vectorization
vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, ngram_range=(1,2))#max_df=0.95: Ignores terms that appear in more than 95% of the documents. This helps remove very common words that might not be informative
#min_df=1: Ignores terms that appear in less than 1 document. This helps remove very rare words that might not be representative.
#ngram_range=(1,2): Considers both single words (unigrams) and two-word combinations (bigrams) as features.
X_train_tfidf = vectorizer.fit_transform(X_train)#: Learns the vocabulary (unique terms) from the training data (X_train).
#transform: Converts the training data into a TF-IDF matrix,
X_test_tfidf = vectorizer.transform(X_test)#This line transforms the test data (X_test) into a TF-IDF matrix using the vocabulary learned from the training data

# Train Logistic Regression model
model = LogisticRegression(max_iter=1000)#max_iter=1000: This parameter sets the maximum number of iterations the solver will run to try and find the optimal solution.
model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)

# Evaluation
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
print('Classification Report:\n', classification_report(y_test, y_pred))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))

# Plot predictions
plt.figure(figsize=(6,4))
plt.hist(y_pred, bins=3, edgecolor='black', alpha=0.7)
plt.xlabel('Predicted Sentiment')
plt.ylabel('Frequency')
plt.title('Distribution of Predicted Sentiments')
plt.xticks(rotation=45)
plt.show()

# Example prediction
def predict_sentiment(review):
    review = clean_text(review)#This line calls the clean_text function (which you defined earlier) to clean the input review text
    review = preprocess_text(review)#This line calls another function, preprocess_text (which you presumably also defined earlier), to further process the cleaned review text
    review_vectorized = vectorizer.transform([review])#his line transforms the preprocessed review text into a numerical representation using the vectorizer object
    prediction = model.predict(review_vectorized)#This line uses the trained model (your Logistic Regression model) to predict the sentiment of the vectorized review
    return prediction[0]

example_review = "The movie was absolutely amazing! I loved it."
print(f"Predicted Sentiment: {predict_sentiment(example_review)}")

